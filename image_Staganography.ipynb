{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image Staganography.ipynb",
      "provenance": [],
      "mount_file_id": "1wCsWeWsV3xTPv7LATCh1XTpod2mog8_C",
      "authorship_tag": "ABX9TyObf28p40nivJ/kiL4TIz5y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitabhardwaj16/Aloha/blob/master/image_Staganography.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1KoiZ5KJBYt"
      },
      "source": [
        "# Imports necessary libraries and modules\n",
        "from itertools import islice\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os \n",
        "import pickle\n",
        "from torchvision import datasets, utils\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from random import shuffle\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrO01lBEJhnG"
      },
      "source": [
        "# Directory path\n",
        "os.chdir(\"..\")\n",
        "cwd = 'content/drive/My Drive/new/train/'\n",
        "\n",
        "# Hyper Parameters\n",
        "num_epochs = 3\n",
        "batch_size = 8\n",
        "learning_rate = 0.0001\n",
        "beta = 0.75\n",
        "\n",
        "# Mean and std deviation of imagenet dataset. Source: http://cs231n.stanford.edu/reports/2017/pdfs/101.pdf\n",
        "std = [0.229, 0.224, 0.225]\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "\n",
        "# TODO: Define train, validation and models\n",
        "MODELS_PATH = 'drive/'\n",
        "# TRAIN_PATH = cwd+'/train/'\n",
        "# VALID_PATH = cwd+'/valid/'\n",
        "VALID_PATH = cwd+''\n",
        "TRAIN_PATH = cwd+''\n",
        "TEST_PATH = cwd+''\n",
        "\n",
        "if not os.path.exists(MODELS_PATH): os.mkdir(MODELS_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEQOY8CxJh3-"
      },
      "source": [
        "def customized_loss(S_prime, C_prime, S, C, B):\n",
        "    ''' Calculates loss specified on the paper.'''\n",
        "    \n",
        "    loss_cover = torch.nn.functional.mse_loss(C_prime, C)\n",
        "    loss_secret = torch.nn.functional.mse_loss(S_prime, S)\n",
        "    loss_all = loss_cover + B * loss_secret\n",
        "    return loss_all, loss_cover, loss_secret\n",
        "\n",
        "def denormalize(image, std, mean):\n",
        "    ''' Denormalizes a tensor of images.'''\n",
        "\n",
        "    for t in range(3):\n",
        "        image[t, :, :] = (image[t, :, :] * std[t]) + mean[t]\n",
        "    return image\n",
        "\n",
        "def imshow(img, idx, learning_rate, beta):\n",
        "    '''Prints out an image given in tensor format.'''\n",
        "    \n",
        "    img = denormalize(img, std, mean)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.title('Example '+str(idx)+', lr='+str(learning_rate)+', B='+str(beta))\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def gaussian(tensor, mean=0, stddev=0.1):\n",
        "    '''Adds random noise to a tensor.'''\n",
        "    \n",
        "    noise = torch.nn.init.normal(torch.Tensor(tensor.size()), 0, 0.1)\n",
        "    return Variable(tensor + noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FtBli9XJh9C"
      },
      "source": [
        "# Preparation Network (2 conv layers)\n",
        "class PrepNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrepNetwork, self).__init__()\n",
        "        self.initialP3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialP4 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialP5 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalP3 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalP4 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalP5 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "\n",
        "    def forward(self, p):\n",
        "        p1 = self.initialP3(p)\n",
        "        p2 = self.initialP4(p)\n",
        "        p3 = self.initialP5(p)\n",
        "        mid = torch.cat((p1, p2, p3), 1)\n",
        "        p4 = self.finalP3(mid)\n",
        "        p5 = self.finalP4(mid)\n",
        "        p6 = self.finalP5(mid)\n",
        "        out = torch.cat((p4, p5, p6), 1)\n",
        "        return out\n",
        "\n",
        "# Hiding Network (5 conv layers)\n",
        "class HidingNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HidingNetwork, self).__init__()\n",
        "        self.initialH3 = nn.Sequential(\n",
        "            nn.Conv2d(153, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialH4 = nn.Sequential(\n",
        "            nn.Conv2d(153, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialH5 = nn.Sequential(\n",
        "            nn.Conv2d(153, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH3 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalH4 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH5 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalH = nn.Sequential(\n",
        "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
        "        \n",
        "    def forward(self, h):\n",
        "        h1 = self.initialH3(h)\n",
        "        h2 = self.initialH4(h)\n",
        "        h3 = self.initialH5(h)\n",
        "        mid = torch.cat((h1, h2, h3), 1)\n",
        "        h4 = self.finalH3(mid)\n",
        "        h5 = self.finalH4(mid)\n",
        "        h6 = self.finalH5(mid)\n",
        "        mid2 = torch.cat((h4, h5, h6), 1)\n",
        "        out = self.finalH(mid2)\n",
        "        out_noise = gaussian(out.data, 0, 0.1)\n",
        "        return out, out_noise\n",
        "\n",
        "# Reveal Network (2 conv layers)\n",
        "class RevealNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RevealNetwork, self).__init__()\n",
        "        self.initialR3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.initialR4 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.initialR5 = nn.Sequential(\n",
        "            nn.Conv2d(3, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR3 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=3, padding=1),\n",
        "            nn.ReLU())\n",
        "        self.finalR4 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=4, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(50, 50, kernel_size=4, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR5 = nn.Sequential(\n",
        "            nn.Conv2d(150, 50, kernel_size=5, padding=2),\n",
        "            nn.ReLU())\n",
        "        self.finalR = nn.Sequential(\n",
        "            nn.Conv2d(150, 3, kernel_size=1, padding=0))\n",
        "\n",
        "    def forward(self, r):\n",
        "        r1 = self.initialR3(r)\n",
        "        r2 = self.initialR4(r)\n",
        "        r3 = self.initialR5(r)\n",
        "        mid = torch.cat((r1, r2, r3), 1)\n",
        "        r4 = self.finalR3(mid)\n",
        "        r5 = self.finalR4(mid)\n",
        "        r6 = self.finalR5(mid)\n",
        "        mid2 = torch.cat((r4, r5, r6), 1)\n",
        "        out = self.finalR(mid2)\n",
        "        return out\n",
        "\n",
        "# Join three networks in one module\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.m1 = PrepNetwork()\n",
        "        self.m2 = HidingNetwork()\n",
        "        self.m3 = RevealNetwork()\n",
        "\n",
        "    def forward(self, secret, cover):\n",
        "        x_1 = self.m1(secret)\n",
        "        mid = torch.cat((x_1, cover), 1)\n",
        "        x_2, x_2_noise = self.m2(mid)\n",
        "        x_3 = self.m3(x_2)\n",
        "        return x_2, x_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLsI5Z_NJiBx"
      },
      "source": [
        "# Creates net object\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gCyEB7sJiGx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "8df3bb1d-00f5-45cb-d7b4-261f8b0f5c0e"
      },
      "source": [
        "# Creates training set\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.ImageFolder(\n",
        "        TRAIN_PATH,\n",
        "        transforms.Compose([\n",
        "        transforms.Scale(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean,\n",
        "        std=std)\n",
        "        ])), batch_size=batch_size, num_workers=1, \n",
        "        pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "# Creates test set\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.ImageFolder(\n",
        "        TEST_PATH, \n",
        "        transforms.Compose([\n",
        "        transforms.Scale(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean,\n",
        "        std=std)\n",
        "        ])), batch_size=2, num_workers=1, \n",
        "        pin_memory=True, shuffle=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNOgrvfxJiL7"
      },
      "source": [
        "def train_model(train_loader, beta, learning_rate):\n",
        "    \n",
        "    # Save optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    loss_history = []\n",
        "    # Iterate over batches performing forward and backward passes\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Train mode\n",
        "        net.train()\n",
        "        \n",
        "        train_losses = []\n",
        "        # Train one epoch\n",
        "        for idx, train_batch in enumerate(train_loader):\n",
        "\n",
        "            data, _  = train_batch\n",
        "\n",
        "            # Saves secret images and secret covers\n",
        "            train_covers = data[:len(data)//2]\n",
        "            train_secrets = data[len(data)//2:]\n",
        "            \n",
        "            # Creates variable from secret and cover images\n",
        "            train_secrets = Variable(train_secrets, requires_grad=False)\n",
        "            train_covers = Variable(train_covers, requires_grad=False)\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            train_hidden, train_output = net(train_secrets, train_covers)\n",
        "\n",
        "            # Calculate loss and perform backprop\n",
        "            train_loss, train_loss_cover, train_loss_secret = customized_loss(train_output, train_hidden, train_secrets, train_covers, beta)\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Saves training loss\n",
        "            train_losses.append(train_loss.item())\n",
        "            loss_history.append(train_loss.item())\n",
        "            \n",
        "            # Prints mini-batch losses\n",
        "            print('Training: Batch {0}/{1}. Loss of {2:.4f}, cover loss of {3:.4f}, secret loss of {4:.4f}'.format(idx+1, len(train_loader), train_loss.item(), train_loss_cover.item(), train_loss_secret.item()))\n",
        "    \n",
        "        torch.save(net.state_dict(), MODELS_PATH+'Epoch N{}.pkl'.format(epoch+1))\n",
        "        \n",
        "        mean_train_loss = np.mean(train_losses)\n",
        "    \n",
        "        # Prints epoch average loss\n",
        "        print ('Epoch [{0}/{1}], Average_loss: {2:.4f}'.format(\n",
        "                epoch+1, num_epochs, mean_train_loss))\n",
        "    \n",
        "    return net, mean_train_loss, loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEE-8WiQJiQu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "903ea80f-747e-4ce3-ed2c-08e64c8b14d3"
      },
      "source": [
        "net, mean_train_loss, loss_history = train_model(train_loader, beta, learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training: Batch 1/626. Loss of 1.8457, cover loss of 1.0485, secret loss of 1.0630\n",
            "Training: Batch 2/626. Loss of 1.3878, cover loss of 0.6846, secret loss of 0.9375\n",
            "Training: Batch 3/626. Loss of 2.0727, cover loss of 1.3535, secret loss of 0.9590\n",
            "Training: Batch 4/626. Loss of 1.7833, cover loss of 1.0827, secret loss of 0.9341\n",
            "Training: Batch 5/626. Loss of 1.7319, cover loss of 1.1244, secret loss of 0.8100\n",
            "Training: Batch 6/626. Loss of 2.2616, cover loss of 1.3827, secret loss of 1.1718\n",
            "Training: Batch 7/626. Loss of 2.6444, cover loss of 1.7363, secret loss of 1.2107\n",
            "Training: Batch 8/626. Loss of 2.0074, cover loss of 0.8580, secret loss of 1.5325\n",
            "Training: Batch 9/626. Loss of 1.6405, cover loss of 0.9297, secret loss of 0.9478\n",
            "Training: Batch 10/626. Loss of 2.1087, cover loss of 1.3728, secret loss of 0.9812\n",
            "Training: Batch 11/626. Loss of 1.4826, cover loss of 0.5135, secret loss of 1.2920\n",
            "Training: Batch 12/626. Loss of 1.4108, cover loss of 0.5681, secret loss of 1.1236\n",
            "Training: Batch 13/626. Loss of 1.6155, cover loss of 1.0392, secret loss of 0.7684\n",
            "Training: Batch 14/626. Loss of 1.8611, cover loss of 1.2585, secret loss of 0.8034\n",
            "Training: Batch 15/626. Loss of 1.8721, cover loss of 1.0735, secret loss of 1.0649\n",
            "Training: Batch 16/626. Loss of 2.2609, cover loss of 1.0730, secret loss of 1.5839\n",
            "Training: Batch 17/626. Loss of 1.7030, cover loss of 0.8777, secret loss of 1.1003\n",
            "Training: Batch 18/626. Loss of 1.3415, cover loss of 0.7230, secret loss of 0.8247\n",
            "Training: Batch 19/626. Loss of 1.4002, cover loss of 0.8622, secret loss of 0.7173\n",
            "Training: Batch 20/626. Loss of 1.6492, cover loss of 0.9577, secret loss of 0.9220\n",
            "Training: Batch 21/626. Loss of 1.3726, cover loss of 0.5613, secret loss of 1.0817\n",
            "Training: Batch 22/626. Loss of 1.3788, cover loss of 0.8146, secret loss of 0.7522\n",
            "Training: Batch 23/626. Loss of 1.6746, cover loss of 0.5843, secret loss of 1.4537\n",
            "Training: Batch 24/626. Loss of 1.4201, cover loss of 0.6548, secret loss of 1.0204\n",
            "Training: Batch 25/626. Loss of 1.9636, cover loss of 0.8203, secret loss of 1.5243\n",
            "Training: Batch 26/626. Loss of 1.2961, cover loss of 0.5897, secret loss of 0.9419\n",
            "Training: Batch 27/626. Loss of 1.4016, cover loss of 0.5299, secret loss of 1.1622\n",
            "Training: Batch 28/626. Loss of 1.2545, cover loss of 0.5644, secret loss of 0.9202\n",
            "Training: Batch 29/626. Loss of 0.7635, cover loss of 0.1755, secret loss of 0.7841\n",
            "Training: Batch 30/626. Loss of 1.4498, cover loss of 0.4949, secret loss of 1.2732\n",
            "Training: Batch 31/626. Loss of 1.2524, cover loss of 0.5415, secret loss of 0.9478\n",
            "Training: Batch 32/626. Loss of 1.1490, cover loss of 0.4643, secret loss of 0.9128\n",
            "Training: Batch 33/626. Loss of 1.0441, cover loss of 0.2386, secret loss of 1.0741\n",
            "Training: Batch 34/626. Loss of 1.3716, cover loss of 0.3367, secret loss of 1.3799\n",
            "Training: Batch 35/626. Loss of 0.7402, cover loss of 0.1634, secret loss of 0.7691\n",
            "Training: Batch 36/626. Loss of 1.1735, cover loss of 0.5750, secret loss of 0.7980\n",
            "Training: Batch 37/626. Loss of 1.0707, cover loss of 0.2593, secret loss of 1.0820\n",
            "Training: Batch 38/626. Loss of 1.2369, cover loss of 0.4333, secret loss of 1.0716\n",
            "Training: Batch 39/626. Loss of 1.0905, cover loss of 0.2083, secret loss of 1.1763\n",
            "Training: Batch 40/626. Loss of 1.0103, cover loss of 0.2025, secret loss of 1.0771\n",
            "Training: Batch 41/626. Loss of 1.8384, cover loss of 0.2625, secret loss of 2.1012\n",
            "Training: Batch 42/626. Loss of 1.1897, cover loss of 0.3747, secret loss of 1.0867\n",
            "Training: Batch 43/626. Loss of 1.4121, cover loss of 0.4513, secret loss of 1.2810\n",
            "Training: Batch 44/626. Loss of 0.7513, cover loss of 0.1603, secret loss of 0.7879\n",
            "Training: Batch 45/626. Loss of 1.1332, cover loss of 0.4084, secret loss of 0.9665\n",
            "Training: Batch 46/626. Loss of 0.6794, cover loss of 0.2204, secret loss of 0.6120\n",
            "Training: Batch 47/626. Loss of 1.2923, cover loss of 0.1982, secret loss of 1.4589\n",
            "Training: Batch 48/626. Loss of 0.8192, cover loss of 0.1089, secret loss of 0.9471\n",
            "Training: Batch 49/626. Loss of 0.8712, cover loss of 0.1883, secret loss of 0.9105\n",
            "Training: Batch 50/626. Loss of 0.8727, cover loss of 0.2131, secret loss of 0.8795\n",
            "Training: Batch 51/626. Loss of 1.0993, cover loss of 0.2431, secret loss of 1.1416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7e8fb043b909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-f7d6660e6738>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, beta, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Calculate loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_cover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustomized_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_secrets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_covers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ2VP7EcJiVy"
      },
      "source": [
        "# Plot loss through epochs\n",
        "plt.plot(loss_history)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-80UGqlaJib0"
      },
      "source": [
        "# net.load_state_dict(torch.load(MODELS_PATH+'Epoch N4.pkl'))\n",
        "\n",
        "# Switch to evaluate mode\n",
        "net.eval()\n",
        "\n",
        "test_losses = []\n",
        "# Show images\n",
        "for idx, test_batch in enumerate(test_loader):\n",
        "     # Saves images\n",
        "    data, _ = test_batch\n",
        "\n",
        "    # Saves secret images and secret covers\n",
        "    test_secret = data[:len(data)//2]\n",
        "    test_cover = data[len(data)//2:]\n",
        "\n",
        "    # Creates variable from secret and cover images\n",
        "    test_secret = Variable(test_secret, volatile=True)\n",
        "    test_cover = Variable(test_cover, volatile=True)\n",
        "\n",
        "    # Compute output\n",
        "    test_hidden, test_output = net(test_secret, test_cover)\n",
        "    \n",
        "    # Calculate loss\n",
        "    test_loss, loss_cover, loss_secret = customized_loss(test_output, test_hidden, test_secret, test_cover, beta)\n",
        "    \n",
        "#     diff_S, diff_C = np.abs(np.array(test_output.data[0]) - np.array(test_secret.data[0])), np.abs(np.array(test_hidden.data[0]) - np.array(test_cover.data[0]))\n",
        "    \n",
        "#     print (diff_S, diff_C)\n",
        "    \n",
        "    if idx in [1,2,3,4]:\n",
        "        print ('Total loss: {:.2f} \\nLoss on secret: {:.2f} \\nLoss on cover: {:.2f}'.format(test_loss.item(), loss_secret.item(), loss_cover.item()))\n",
        "\n",
        "        # Creates img tensor\n",
        "        imgs = [test_secret.data, test_output.data, test_cover.data, test_hidden.data]\n",
        "        imgs_tsor = torch.cat(imgs, 0)\n",
        "\n",
        "        # Prints Images\n",
        "        imshow(utils.make_grid(imgs_tsor), idx+1, learning_rate=learning_rate, beta=beta)\n",
        "        \n",
        "    test_losses.append(test_loss.item())\n",
        "        \n",
        "mean_test_loss = np.mean(test_losses)\n",
        "\n",
        "print ('Average loss on test set: {:.2f}'.format(mean_test_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}